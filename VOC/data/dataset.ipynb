{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T07:08:20.286968Z",
     "start_time": "2019-12-24T07:08:20.280953Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-67b1c7746edc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCVTransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxywh2xyxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxyxy2xywh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletterbox_resize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_general\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from utils import CVTransform, xywh2xyxy, xyxy2xywh, letterbox_resize, iou_general, plot\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from V2.VOC.utils import parse_anchors\n",
    "from pathlib import Path\n",
    "from utils import traverse_voc\n",
    "import pickle\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2007:\n",
    "    def __init__(self, opt):\n",
    "        self.label_names = opt.VOC_BBOX_LABEL_NAMES\n",
    "        self.label_names_dict = {name: index for index, name in enumerate(self.label_names)}\n",
    "        self.ann_dir = Path(opt.data_dir) / 'Annotations'\n",
    "        self.img_dir = Path(opt.data_dir) / 'JPEGImages'\n",
    "        self.obj_dict_path = opt.obj_path\n",
    "        if not Path(self.obj_dict_path).exists():\n",
    "            traverse_voc(self.ann_dir, self.obj_dict_path)\n",
    "        self.obj_dicts = pickle.load(open(self.obj_dict_path, 'rb'))\n",
    "        self.filenames = [_ for _ in self.obj_dicts.keys()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def get_example(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        obj_dict = self.obj_dicts[filename]\n",
    "        obj_boxes = obj_dict['boxes']\n",
    "        obj_names = obj_dict['names']\n",
    "        obj_labels = [self.label_names_dict[name] for name in obj_names]\n",
    "        img_path = self.img_dir / f'{filename}'\n",
    "        img = io.imread(img_path)\n",
    "        return img, np.array(obj_labels), np.asarray(obj_boxes)\n",
    "\n",
    "\n",
    "def pytorch_normailze(img, mean, std):\n",
    "    torch_normailze = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "    img = torch_normailze(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "class VOC2007Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    :return\n",
    "    training:\n",
    "        1.img:(batch_size,3,448,448)/tensor\n",
    "        2.gt_bbox:(batch_size,-1,4)/tensor\n",
    "        3.gt_label:(batch_size,-1)/ndarray\n",
    "        4.scale:(batch_size,1,2)/ndarray\n",
    "        5.y_true['target']:(13,13,5,25)/tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.database = VOC2007(opt)\n",
    "        # anchor's scale is 416 / shape: [5, 2]\n",
    "        self.anchor_base = parse_anchors(opt.anchors_path, opt)\n",
    "        self.image_aug = CVTransform(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.database)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, labels, boxes = self.database.get_example(index)\n",
    "        img, labels, boxes = self.image_aug(img, boxes, labels, 'RGB')\n",
    "        resized_img, resized_boxes = letterbox_resize(img, boxes, [self.opt.img_h, self.opt.img_w])\n",
    "        target = self.make_target(resized_boxes, labels, self.opt.img_size)\n",
    "        img_norm = pytorch_normailze(resized_img, self.opt.mean, self.opt.std)\n",
    "        return img_norm, target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
